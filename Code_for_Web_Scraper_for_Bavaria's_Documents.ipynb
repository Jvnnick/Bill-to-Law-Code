{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qeIx2J89ExDY"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup, NavigableString\n",
        "import os\n",
        "import time\n",
        "import re\n",
        "\n",
        "def sanitize_filename(name):\n",
        "    for char in ['/', '\\\\', ':', '*', '?', '\"', '<', '>', '|']:\n",
        "        name = name.replace(char, '_')\n",
        "    return name\n",
        "\n",
        "def download_pdf_from_beratungsverlauf(beratungsverlauf_url, folder_path, downloaded_pdfs):\n",
        "    response = requests.get(beratungsverlauf_url)\n",
        "    if response.status_code != 200:\n",
        "        print(f'Failed to retrieve the Beratungsverlauf page: {beratungsverlauf_url}')\n",
        "        return\n",
        "\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Gather PDF links\n",
        "    pdf_links = set()  # Using a set to store unique PDF links\n",
        "    for pdf_link in soup.find_all('a', href=lambda href: href and href.endswith('.pdf')):\n",
        "        pdf_url = pdf_link['href']\n",
        "\n",
        "        # Get the parent <td> containing the PDF information\n",
        "        parent_td = pdf_link.find_parent('td')\n",
        "\n",
        "        # Try to find the title immediately before the PDF link\n",
        "        proposed_pdf_name = None\n",
        "\n",
        "        # Navigate through previous siblings until we find the relevant text\n",
        "        for sibling in pdf_link.previous_siblings:\n",
        "            if isinstance(sibling, NavigableString):\n",
        "                text = sibling.strip()\n",
        "                if text and not text.lower().startswith(\"download pdf\"):\n",
        "                    proposed_pdf_name = text  # Capture the relevant text\n",
        "                    break\n",
        "\n",
        "            if sibling.name == 'br':\n",
        "                continue  # Ignore <br> elements\n",
        "\n",
        "            # If we find a tag that is not <br> then we extract its text\n",
        "            if sibling.string and sibling.string.strip():\n",
        "                proposed_pdf_name = sibling.string.strip()\n",
        "                break  # Stop as we found a valid name\n",
        "\n",
        "        # If we couldn't find a name just above it, we can also inspect the content in the parent <td>\n",
        "        if proposed_pdf_name is None:\n",
        "            td_text = parent_td.get_text(separator=\"<br>\", strip=True).split(\"<br>\")\n",
        "            # Check the part that contains the intended name if applicable\n",
        "            for text in td_text:\n",
        "                if \"Download PDF\" not in text and text.strip():\n",
        "                    proposed_pdf_name = text.strip()\n",
        "                    break\n",
        "\n",
        "        # Generate the PDF name or fallback to unnamed if nothing is found\n",
        "        if proposed_pdf_name:\n",
        "            pdf_name = sanitize_filename(proposed_pdf_name + \".pdf\")\n",
        "        else:\n",
        "            pdf_name = \"Unnamed.pdf\"  # Fallback name\n",
        "\n",
        "        # Prepare the URL\n",
        "        if not pdf_url.startswith(('http:', 'https:')):\n",
        "            pdf_url = requests.compat.urljoin(beratungsverlauf_url, pdf_url)\n",
        "\n",
        "        # Only add to the pdf_links if the name is valid\n",
        "        if pdf_name != \"Unnamed.pdf\":\n",
        "            pdf_links.add((pdf_url, pdf_name))  # Add to the set to ensure uniqueness\n",
        "\n",
        "    # Attempt to download each unique PDF link\n",
        "    for pdf_url, pdf_name in pdf_links:\n",
        "        print(f'Attempting to download PDF: {pdf_name} from {pdf_url}')  # Debug print\n",
        "        if pdf_name != \"Unnamed.pdf\":  # Ensure we skip unnamed files\n",
        "            if pdf_url not in downloaded_pdfs:\n",
        "                downloaded_pdfs.add(pdf_url)  # Register the download\n",
        "                download_pdf(pdf_url, folder_path, pdf_name)  # Call the download function\n",
        "            else:\n",
        "                print(f'Skipping duplicate PDF: {pdf_url}')  # Debug print\n",
        "        else:\n",
        "            print(f'Skipping unnamed PDF: {pdf_url}')  # Debug print\n",
        "\n",
        "        time.sleep(4)  # Add a delay after each PDF download\n",
        "\n",
        "def download_pdf(pdf_url, folder_path, pdf_name):\n",
        "    \"\"\"Download the PDF file.\"\"\"\n",
        "    if not os.path.exists(folder_path):\n",
        "        os.makedirs(folder_path)  # Create the folder if it doesn't exist\n",
        "\n",
        "    pdf_file_path = os.path.join(folder_path, pdf_name)\n",
        "\n",
        "    # Manage existing filenames using a counter\n",
        "    counter = 1\n",
        "    while os.path.exists(pdf_file_path):\n",
        "        base_name, extension = os.path.splitext(pdf_name)\n",
        "        pdf_file_path = os.path.join(folder_path, f\"{base_name}_{counter}{extension}\")\n",
        "        counter += 1\n",
        "\n",
        "    response = requests.get(pdf_url)\n",
        "    if response.status_code == 200:\n",
        "        with open(pdf_file_path, 'wb') as pdf_file:\n",
        "            pdf_file.write(response.content)\n",
        "        print(f'Downloaded: {pdf_name} in {folder_path}')\n",
        "    else:\n",
        "        print(f'Failed to download PDF: {pdf_url} with status code: {response.status_code}')\n",
        "\n",
        "def extract_pdfs_from_results(search_url, base_folder_path):\n",
        "    current_page = 1\n",
        "    base_search_url = 'https://www.bayern.landtag.de/parlament/dokumente/drucksachen?isInitialCheck=0&;q=&dknr=&suchverhalten=AND&dokumentenart=Drucksache&ist_basisdokument=off&sort=date&anzahl_treffer=100&wahlperiodeid%5B0%5D=19&wahlperiodeid%5B1%5D=18&wahlperiodeid%5B2%5D=17&wahlperiodeid%5B3%5D=16&wahlperiodeid%5B4%5D=15&wahlperiodeid%5B5%5D=14&wahlperiodeid%5B6%5D=13&wahlperiodeid%5B7%5D=12&wahlperiodeid%5B8%5D=11&wahlperiodeid%5B9%5D=10&wahlperiodeid%5B10%5D=9&wahlperiodeid%5B11%5D=8&wahlperiodeid%5B12%5D=7&wahlperiodeid%5B13%5D=6&erfassungsdatum%5Bstart%5D=&erfassungsdatum%5Bend%5D=&suchvorgangsarten%5B0%5D=Gesetze%5C%5CGesetzentwurf&suchvorgangsarten%5B1%5D=Gesetze%5C%5CHaushaltsgesetz,%20Nachtragshaushaltsgesetz&suchvorgangsarten%5B2%5D=Gesetze%5C%5CStaatsvertrag&gremium%5B0%5D=Ausschuss%20f%C3%BCr%20Landesentwicklung%20und%20Umweltfragen&gremium%5B1%5D=Ausschuss%20f%C3%BCr%20Umwelt%20und%20Verbraucherschutz&gremium%5B2%5D=Ausschuss%20f%C3%BCr%20Umwelt%20und%20Gesundheit&dlh=null'  # Your base URL here\n",
        "    downloaded_pdfs = set()  # Set to track downloaded PDFs\n",
        "\n",
        "    while True:\n",
        "        search_url = f\"{base_search_url}&page={current_page}\"\n",
        "        print(f'Retrieving page {current_page}:{search_url}')\n",
        "\n",
        "        response = requests.get(search_url)\n",
        "        if response.status_code != 200:\n",
        "            print(f'Failed to retrieve the search results for page {current_page}.')\n",
        "            break\n",
        "\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        results = soup.find_all('div', class_='row result')\n",
        "        if not results:\n",
        "            print('No more results found or last page reached.')\n",
        "            break\n",
        "\n",
        "        print(f'Processing page {current_page}, found {len(results)} results.')\n",
        "\n",
        "        for result in results:\n",
        "            links = result.find_all('a', class_='link-with-icon')\n",
        "            for link in links:\n",
        "                if \"vorgangsanzeige\" in link['href']:\n",
        "                    beratungsverlauf_url = link['href']\n",
        "                    print('Found Beratungsverlauf link:', beratungsverlauf_url)\n",
        "\n",
        "                    # Fetch the Beratungsverlauf page to extract the \"betreff\" text and WP\n",
        "                    beratungsverlauf_response = requests.get(beratungsverlauf_url)\n",
        "                    if beratungsverlauf_response.status_code != 200:\n",
        "                        print(f'Failed to retrieve Beratungsverlauf page: {beratungsverlauf_url}')\n",
        "                        continue\n",
        "\n",
        "                    # Parse the Beratungsverlauf page content\n",
        "                    beratungsverlauf_soup = BeautifulSoup(beratungsverlauf_response.content, 'html.parser')\n",
        "\n",
        "                    # Find the basistext span\n",
        "                    basistext_span = beratungsverlauf_soup.find('span', id='basistext')\n",
        "                    legislative_period = extract_legislative_period(basistext_span.get_text(strip=True)) if basistext_span else 'Unknown_WP'\n",
        "\n",
        "                    # Create a subfolder for the legislative period (Wahlperioden)\n",
        "                    period_folder_path = os.path.join(base_folder_path, legislative_period)\n",
        "                    os.makedirs(period_folder_path, exist_ok=True)  # Create the main legislative period folder\n",
        "\n",
        "                    # Limit the length of the folder name\n",
        "                    betreff_span = beratungsverlauf_soup.find('span', id='betreff')\n",
        "                    folder_name = betreff_span.get_text(strip=True).replace('/', '_').replace('\\\\', '_')[:75]  # Sanitize folder name\n",
        "\n",
        "                    # Add a unique identifier (e.g., ID from the URL) to the folder name and complete the final folder path for the Gesetzentwurf\n",
        "                    unique_identifier = beratungsverlauf_url.split('=')[-1]\n",
        "                    gesetzentwurf_folder_path = os.path.join(period_folder_path, f\"{folder_name}_{unique_identifier}\")\n",
        "                    os.makedirs(gesetzentwurf_folder_path, exist_ok=True)  # Create the Gesetzentwurf folder\n",
        "\n",
        "                    print(f'Creating Gesetzentwurf folder: {gesetzentwurf_folder_path}')  # Debug statement\n",
        "\n",
        "                    # Call download_pdf_from_beratungsverlauf with downloaded_pdfs\n",
        "                    download_pdf_from_beratungsverlauf(beratungsverlauf_url, gesetzentwurf_folder_path, downloaded_pdfs)\n",
        "                    time.sleep(4)  # Take a 4 second break between downloads\n",
        "                    break  # Exit after finding the first valid link\n",
        "            else:\n",
        "                print('No valid Beratungsverlauf link in this result.')\n",
        "\n",
        "        current_page += 1  # Move to the next page after processing this one\n",
        "\n",
        "def extract_legislative_period(basistext):\n",
        "    # Use regular expressions to extract the WP number\n",
        "    match = re.search(r'Nr\\. (\\d+)', basistext)\n",
        "    if match:\n",
        "        return f'WP_{match.group(1)}'  # Format as 'WP_19'\n",
        "    return 'WP_Unknown'  # Fallback for unknown periods\n",
        "\n",
        "def main():\n",
        "    search_url = 'https://www.bayern.landtag.de/parlament/dokumente/drucksachen?isInitialCheck=0&;q=&dknr=&suchverhalten=AND&dokumentenart=Drucksache&ist_basisdokument=off&sort=date&anzahl_treffer=100&wahlperiodeid%5B%5D=19&wahlperiodeid%5B%5D=18&wahlperiodeid%5B%5D=17&wahlperiodeid%5B%5D=16&wahlperiodeid%5B%5D=15&wahlperiodeid%5B%5D=14&wahlperiodeid%5B%5D=13&wahlperiodeid%5B%5D=12&wahlperiodeid%5B%5D=11&wahlperiodeid%5B%5D=10&wahlperiodeid%5B%5D=9&wahlperiodeid%5B%5D=8&wahlperiodeid%5B%5D=7&wahlperiodeid%5B%5D=6&erfassungsdatum%5Bstart%5D=&erfassungsdatum%5Bend%5D=&dokumentenart=Drucksache&suchvorgangsarten%5B%5D=Gesetze%5C%5CGesetzentwurf&suchvorgangsarten%5B%5D=Gesetze%5C%5CHaushaltsgesetz%2C+Nachtragshaushaltsgesetz&suchvorgangsarten%5B%5D=Gesetze%5C%5CStaatsvertrag&gremium%5B%5D=Ausschuss+f%C3%BCr+Landesentwicklung+und+Umweltfragen&gremium%5B%5D=Ausschuss+f%C3%BCr+Umwelt+und+Verbraucherschutz&gremium%5B%5D=Ausschuss+f%C3%BCr+Umwelt+und+Gesundheit&dlh=null'\n",
        "    base_search_url = 'https://www.bayern.landtag.de/parlament/dokumente/drucksachen?isInitialCheck=0&;q=&dknr=&suchverhalten=AND&dokumentenart=Drucksache&ist_basisdokument=off&sort=date&anzahl_treffer=100&wahlperiodeid%5B0%5D=19&wahlperiodeid%5B1%5D=18&wahlperiodeid%5B2%5D=17&wahlperiodeid%5B3%5D=16&wahlperiodeid%5B4%5D=15&wahlperiodeid%5B5%5D=14&wahlperiodeid%5B6%5D=13&wahlperiodeid%5B7%5D=12&wahlperiodeid%5B8%5D=11&wahlperiodeid%5B9%5D=10&wahlperiodeid%5B10%5D=9&wahlperiodeid%5B11%5D=8&wahlperiodeid%5B12%5D=7&wahlperiodeid%5B13%5D=6&erfassungsdatum%5Bstart%5D=&erfassungsdatum%5Bend%5D=&suchvorgangsarten%5B0%5D=Gesetze%5C%5CGesetzentwurf&suchvorgangsarten%5B1%5D=Gesetze%5C%5CHaushaltsgesetz,%20Nachtragshaushaltsgesetz&suchvorgangsarten%5B2%5D=Gesetze%5C%5CStaatsvertrag&gremium%5B0%5D=Ausschuss%20f%C3%BCr%20Landesentwicklung%20und%20Umweltfragen&gremium%5B1%5D=Ausschuss%20f%C3%BCr%20Umwelt%20und%20Verbraucherschutz&gremium%5B2%5D=Ausschuss%20f%C3%BCr%20Umwelt%20und%20Gesundheit&dlh=null'\n",
        "    base_folder_path = 'Beratungsverl√§ufe_Umweltausschuss_Bayern'\n",
        "    extract_pdfs_from_results(search_url, base_folder_path)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}